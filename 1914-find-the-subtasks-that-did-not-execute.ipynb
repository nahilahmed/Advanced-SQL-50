{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a href=\"https://leetcode.com/problems/find-the-subtasks-that-did-not-execute\">1914. Find the Subtasks That Did Not Execute</a></h2><h3>Hard</h3><hr><p>Table: <code>Tasks</code></p>\n",
    "\n",
    "<pre>\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| task_id        | int     |\n",
    "| subtasks_count | int     |\n",
    "+----------------+---------+\n",
    "task_id is the column with unique values for this table.\n",
    "Each row in this table indicates that task_id was divided into subtasks_count subtasks labeled from 1 to subtasks_count.\n",
    "It is guaranteed that 2 &lt;= subtasks_count &lt;= 20.\n",
    "</pre>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p>Table: <code>Executed</code></p>\n",
    "\n",
    "<pre>\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| task_id       | int     |\n",
    "| subtask_id    | int     |\n",
    "+---------------+---------+\n",
    "(task_id, subtask_id) is the combination of columns with unique values for this table.\n",
    "Each row in this table indicates that for the task task_id, the subtask with ID subtask_id was executed successfully.\n",
    "It is <strong>guaranteed</strong> that subtask_id &lt;= subtasks_count for each task_id.</pre>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "\n",
    "<p>Write a solution&nbsp;to report the IDs of the missing subtasks for each <code>task_id</code>.</p>\n",
    "\n",
    "<p>Return the result table in <strong>any order</strong>.</p>\n",
    "\n",
    "<p>The result format is in the following example.</p>\n",
    "\n",
    "<p>&nbsp;</p>\n",
    "<p><strong class=\"example\">Example 1:</strong></p>\n",
    "\n",
    "<pre>\n",
    "<strong>Input:</strong> \n",
    "Tasks table:\n",
    "+---------+----------------+\n",
    "| task_id | subtasks_count |\n",
    "+---------+----------------+\n",
    "| 1       | 3              |\n",
    "| 2       | 2              |\n",
    "| 3       | 4              |\n",
    "+---------+----------------+\n",
    "Executed table:\n",
    "+---------+------------+\n",
    "| task_id | subtask_id |\n",
    "+---------+------------+\n",
    "| 1       | 2          |\n",
    "| 3       | 1          |\n",
    "| 3       | 2          |\n",
    "| 3       | 3          |\n",
    "| 3       | 4          |\n",
    "+---------+------------+\n",
    "<strong>Output:</strong> \n",
    "+---------+------------+\n",
    "| task_id | subtask_id |\n",
    "+---------+------------+\n",
    "| 1       | 1          |\n",
    "| 1       | 3          |\n",
    "| 2       | 1          |\n",
    "| 2       | 2          |\n",
    "+---------+------------+\n",
    "<strong>Explanation:</strong> \n",
    "Task 1 was divided into 3 subtasks (1, 2, 3). Only subtask 2 was executed successfully, so we include (1, 1) and (1, 3) in the answer.\n",
    "Task 2 was divided into 2 subtasks (1, 2). No subtask was executed successfully, so we include (2, 1) and (2, 2) in the answer.\n",
    "Task 3 was divided into 4 subtasks (1, 2, 3, 4). All of the subtasks were executed successfully.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/23 16:23:23 WARN Utils: Your hostname, Nahils-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.103 instead (on interface en0)\n",
      "25/02/23 16:23:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/23 16:23:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/23 16:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|task_id|subtasks_count|\n",
      "+-------+--------------+\n",
      "|      1|             3|\n",
      "|      2|             2|\n",
      "|      3|             4|\n",
      "+-------+--------------+\n",
      "\n",
      "+-------+----------+\n",
      "|task_id|subtask_id|\n",
      "+-------+----------+\n",
      "|      1|         2|\n",
      "|      3|         1|\n",
      "|      3|         2|\n",
      "|      3|         3|\n",
      "|      3|         4|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SQLPractice\").getOrCreate()\n",
    "\n",
    "# Create the Tasks DataFrame\n",
    "tasks_data = [\n",
    "    (1, 3),\n",
    "    (2, 2),\n",
    "    (3, 4)\n",
    "]\n",
    "tasks_columns = [\"task_id\", \"subtasks_count\"]\n",
    "tasks_df = spark.createDataFrame(tasks_data, schema=tasks_columns)\n",
    "\n",
    "# Create the Executed DataFrame\n",
    "executed_data = [\n",
    "    (1, 2),\n",
    "    (3, 1),\n",
    "    (3, 2),\n",
    "    (3, 3),\n",
    "    (3, 4)\n",
    "]\n",
    "executed_columns = [\"task_id\", \"subtask_id\"]\n",
    "executed_df = spark.createDataFrame(executed_data, schema=executed_columns)\n",
    "\n",
    "# Register the DataFrames as temporary views for SQL queries\n",
    "tasks_df.createOrReplaceTempView(\"Tasks\")\n",
    "executed_df.createOrReplaceTempView(\"Executed\")\n",
    "\n",
    "tasks_df.show()\n",
    "executed_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|task_id|subtask_id|\n",
      "+-------+----------+\n",
      "|      1|         3|\n",
      "|      1|         1|\n",
      "|      2|         2|\n",
      "|      2|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL to create a range of values from 1 to 20\n",
    "spark.sql(\"\"\"\n",
    "    WITH subtask_id_table AS (\n",
    "        SELECT explode(array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)) AS subtask_id\n",
    "    ),\n",
    "          \n",
    "    all_tasks_table AS (\n",
    "        SELECT t.task_id, st.subtask_id\n",
    "            FROM Tasks t\n",
    "            INNER JOIN subtask_id_table st ON st.subtask_id <= t.subtasks_count \n",
    "    )\n",
    "          \n",
    "    SELECT * FROM all_tasks_table\n",
    "    EXCEPT\n",
    "    SELECT * FROM Executed\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
